{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8f4423-f686-49a1-8dd6-c66df5fa1732",
   "metadata": {},
   "source": [
    "# Business problem understanding\n",
    "**Automated identification of smoking behavior from images can play a vital role in public health surveillance, green environment initiatives, and smart city monitoring. This project aims to develop a deep learning model using CNN to distinguish between smokers and non-smokers in diverse real-world scenarios. Accurate classification enables scalable screening, reduces manual monitoring effort, and supports health policy enforcement and anti-smoking campaigns through intelligent surveillance systems.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b27a34e-b792-4f6d-9080-1f5227f61fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc9c03-662d-43b3-ad85-16dc9ecc26f2",
   "metadata": {},
   "source": [
    "# DATA UNDERSTANDING \n",
    "1. they have given multiple images of smoker and nonsmoker\n",
    "2. smoker images are in 1 floder and nonsmoker images are in 1 folder\n",
    "3. since,we have 2 classes, its a binary image classificatio project\n",
    "4. the given images are of different sizes(different shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41381d24-b776-4321-884f-ca6260b98868",
   "metadata": {},
   "source": [
    "# DATA(IMAGE) PREPROCESSING\n",
    "**LOAD THE IMAGE DATA AND IMAGE DATA PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28382c8b-4b8f-42ea-a82c-3f9db36adfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b997512a-f952-4653-b2bb-edd5a574abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataGenerator(rescale = 1/255)# PIXEL INTENSITY BOUGHT BETWEEN 0 TO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a595a780-d7ea-417d-848b-921b44bd3b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 716 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_data.flow_from_directory(r\"C:\\Users\\L.RAMYA\\Downloads\\New folder\\Training\",\n",
    "                                              target_size = (250,250),\n",
    "                                              class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d617023-19f9-49bc-a468-e64e7809209e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non-smoker': 0, 'smoker': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93bc67a1-d352-4c76-a5d2-a1cb375326f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 224 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "test_set = test_data.flow_from_directory(r\"C:\\Users\\L.RAMYA\\Downloads\\New folder\\Testing\",\n",
    "                                         target_size = (250,250),\n",
    "                                         class_mode = \"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef2c59-545f-4cc0-a373-5754e0ac5fac",
   "metadata": {},
   "source": [
    "# MODELLING - CONVOLUTION NEURAL NETWORK\n",
    "**initialising the CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f405e529-19f0-4bee-b217-9abc448c3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "Classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c0aed-8f49-43a8-8735-ecb05f0545ae",
   "metadata": {},
   "source": [
    "**step -1 CONVOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f16e59-81f4-4640-b7ba-a55cda9fad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "\n",
    "Classifier.add(Conv2D(input_shape = [250,250,3],      #3-means color image if black and white not need to give any number \n",
    "                      filters = 32,                # max no.of filters\n",
    "                      kernel_size = 3,            # fixed\n",
    "                      activation = 'relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9af8fa6-b35b-4049-a104-1ffb44ed3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "\n",
    "Classifier.add(Conv2D(input_shape = [250,250,3],      #3-means color image if black and white not need to give any number \n",
    "                      filters = 32,                # max no.of filters\n",
    "                      kernel_size = 3,            # fixed\n",
    "                      activation = 'relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c8486-aceb-4557-9458-156e0f65c208",
   "metadata": {},
   "source": [
    "**step -2 MAX POOLING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e49a64-9ed9-484b-acbc-18eb27e2dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "Classifier.add(MaxPooling2D(pool_size=2,strides=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c127c381-fa56-485f-9a9b-b748f1b64e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "Classifier.add(MaxPooling2D(pool_size=2,strides=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edd39c-6f46-4e9d-867e-f1adf41598e9",
   "metadata": {},
   "source": [
    "**step -3 FLATTENING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d896082-b6ca-4844-821c-ea4e24b3dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "Classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e44a43-882b-4da6-8822-c53e60b605fe",
   "metadata": {},
   "source": [
    "**step -4 FULL CONNECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb68b7de-6efb-40c6-857c-2a456581c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "# 1st hidden layer with 128 neurons  (no.of hidden layers =(1,2n)=(1,2*64)=(1,168)\n",
    "Classifier.add(Dense(units = 128,activation = 'relu'))\n",
    "\n",
    "# output layer with 1 neuron\n",
    "Classifier.add(Dense(units = 1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7f1a6-726f-4ada-860c-166fa5cad749",
   "metadata": {},
   "source": [
    "**Training the CNN mode1 with train data and testing the model with test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a4c52f-0e60-4702-a211-608d27cdbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier.compile(optimizer = 'adam',\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10cf22f5-b222-45e6-9516-0403a5e60f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 965ms/step - accuracy: 0.5614 - loss: 1.7029 - val_accuracy: 0.6741 - val_loss: 0.6076\n",
      "Epoch 2/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 735ms/step - accuracy: 0.7476 - loss: 0.5629 - val_accuracy: 0.7009 - val_loss: 0.6049\n",
      "Epoch 3/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 756ms/step - accuracy: 0.8014 - loss: 0.4360 - val_accuracy: 0.7054 - val_loss: 0.5815\n",
      "Epoch 4/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 764ms/step - accuracy: 0.9225 - loss: 0.2439 - val_accuracy: 0.6920 - val_loss: 0.6046\n",
      "Epoch 5/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 840ms/step - accuracy: 0.9585 - loss: 0.1436 - val_accuracy: 0.6830 - val_loss: 0.7205\n",
      "Epoch 6/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 867ms/step - accuracy: 0.9964 - loss: 0.0360 - val_accuracy: 0.6964 - val_loss: 0.8561\n",
      "Epoch 7/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 819ms/step - accuracy: 1.0000 - loss: 0.0136 - val_accuracy: 0.6830 - val_loss: 0.9279\n",
      "Epoch 8/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 970ms/step - accuracy: 0.9975 - loss: 0.0118 - val_accuracy: 0.7411 - val_loss: 0.9284\n",
      "Epoch 9/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 0.6830 - val_loss: 1.0538\n",
      "Epoch 10/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.7143 - val_loss: 1.0956\n",
      "Epoch 11/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 809ms/step - accuracy: 1.0000 - loss: 8.4062e-04 - val_accuracy: 0.7188 - val_loss: 1.1561\n",
      "Epoch 12/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 836ms/step - accuracy: 1.0000 - loss: 5.0577e-04 - val_accuracy: 0.7098 - val_loss: 1.1806\n",
      "Epoch 13/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 808ms/step - accuracy: 1.0000 - loss: 3.6836e-04 - val_accuracy: 0.7188 - val_loss: 1.2101\n",
      "Epoch 14/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 892ms/step - accuracy: 1.0000 - loss: 3.2934e-04 - val_accuracy: 0.7054 - val_loss: 1.2201\n",
      "Epoch 15/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 904ms/step - accuracy: 1.0000 - loss: 2.5652e-04 - val_accuracy: 0.7188 - val_loss: 1.2386\n",
      "Epoch 16/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 809ms/step - accuracy: 1.0000 - loss: 2.2825e-04 - val_accuracy: 0.7098 - val_loss: 1.2476\n",
      "Epoch 17/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 798ms/step - accuracy: 1.0000 - loss: 2.1694e-04 - val_accuracy: 0.7009 - val_loss: 1.2555\n",
      "Epoch 18/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 813ms/step - accuracy: 1.0000 - loss: 1.8812e-04 - val_accuracy: 0.7143 - val_loss: 1.2779\n",
      "Epoch 19/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 759ms/step - accuracy: 1.0000 - loss: 1.7131e-04 - val_accuracy: 0.7188 - val_loss: 1.2827\n",
      "Epoch 20/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 755ms/step - accuracy: 1.0000 - loss: 1.3397e-04 - val_accuracy: 0.7143 - val_loss: 1.2923\n",
      "Epoch 21/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 751ms/step - accuracy: 1.0000 - loss: 1.2439e-04 - val_accuracy: 0.7143 - val_loss: 1.2975\n",
      "Epoch 22/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 750ms/step - accuracy: 1.0000 - loss: 1.1493e-04 - val_accuracy: 0.7143 - val_loss: 1.3076\n",
      "Epoch 23/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 771ms/step - accuracy: 1.0000 - loss: 1.1476e-04 - val_accuracy: 0.7143 - val_loss: 1.3171\n",
      "Epoch 24/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 803ms/step - accuracy: 1.0000 - loss: 9.6679e-05 - val_accuracy: 0.7098 - val_loss: 1.3223\n",
      "Epoch 25/25\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 934ms/step - accuracy: 1.0000 - loss: 8.8164e-05 - val_accuracy: 0.7098 - val_loss: 1.3327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cacdde8080>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classifier.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8af7ab-b362-483b-b033-fb8277daa067",
   "metadata": {},
   "source": [
    "# EVALUTION\n",
    "**MAKING A SINGLE PREDICTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee70a102-e71b-4415-bae8-2c42bfdb0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97dc0e1c-4b45-49d2-ae98-d60040458bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "Smoker\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "test_image = Image.open(r\"C:\\Users\\L.RAMYA\\Downloads\\New folder\\Validation\\smoking_0311.jpg\")\n",
    "\n",
    "# Data preprocessing\n",
    "test_image = test_image.resize((250, 250))  # Resize to match your CNN input\n",
    "test_image = test_image.convert(\"RGB\")      # Ensure 3 channels\n",
    "test_image = np.array(test_image) / 255.0   # Normalize pixel values\n",
    "test_image = np.expand_dims(test_image, axis=0)  # Add batch dimension\n",
    "\n",
    "# Prediction\n",
    "result = Classifier.predict(test_image)\n",
    "\n",
    "# Interpretation\n",
    "if result[0][0] > 0.5:\n",
    "    print('Smoker')\n",
    "else:\n",
    "    print('Non-Smoker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31d24dda-8409-4ef5-8ff1-8f4c85a297b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_classification_model.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming 'model' is your trained classifier\n",
    "joblib.dump(Classifier, 'image_classification_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3cd07-f9ad-416d-9145-f7f133577366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
